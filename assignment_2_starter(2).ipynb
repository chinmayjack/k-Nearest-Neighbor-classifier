{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_2_starter.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "8TtFxZ8LMJ8f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is the starter code for part two of assignment two."
      ]
    },
    {
      "metadata": {
        "id": "1M2ciAUtFYYS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "tfe = tf.contrib.eager\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "skQBAMSgFgJF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "# Dataset will be cached locally after you run this code\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# These types are required for the operation we use to compute\n",
        "# loss. Omit, and you shall receive a cryptic error message.\n",
        "y_train = y_train.astype(np.int32)\n",
        "y_test = y_test.astype(np.int32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_phAjMlEFsA0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "buffer_size = 5000\n",
        "batch_size = 100\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(buffer_size)\n",
        "train_dataset = train_dataset.batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "acHRNledFwXL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Model, self).__init__()\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    # FIX ME\n",
        "    # add some layers to your model\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.flatten(x)\n",
        "    # FIX ME\n",
        "    # use your layers (don't forget to add activation functions here as well\n",
        "    # if you haven't specified them in your layer definintions)\n",
        "    return x # be sure to return logits, not softmax output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TBOQC-VPK9nZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oi0611F5GFyK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss(logits, labels):\n",
        "  # FIX ME\n",
        "  # You will need to modify this function, of course.\n",
        "  # Best bet, use tf.nn.sparse_softmax_cross_entropy_with_logits\n",
        "  # though if you're interested, you can write your own.\n",
        "  return tf.constant(np.random.random())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2vaut5jVGuwP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_accuracy(logits, labels):\n",
        "  # You shoud not need to modify this function\n",
        "  predictions = tf.argmax(logits, axis=1)\n",
        "  batch_size = int(logits.shape[0])\n",
        "  return tf.reduce_sum(\n",
        "      tf.cast(tf.equal(predictions, labels), dtype=tf.float32)) / batch_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wxM3LBNRGLdS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(model, images, labels):\n",
        "  # You should not need to modify this function\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(images)\n",
        "    loss_value = loss(logits, labels)\n",
        "    \n",
        "  grads = tape.gradient(loss_value, model.variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.variables))\n",
        "  \n",
        "  return loss_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FncXV6IzG-In",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The first time you run the below block it will crash\n",
        "# with an error 'ValueError: No variables provided.'\n",
        "# This is because the call method of your model\n",
        "# is not using any trainable variables.\n",
        "\n",
        "model = Model()\n",
        "\n",
        "epochs = 10\n",
        "step_counter = 0\n",
        "\n",
        "for epoch_n in range(epochs):\n",
        "  print('Epoch #%d' % (epoch_n))\n",
        "  for (batch, (images, labels)) in enumerate(train_dataset):\n",
        "    loss_value = train(model, images, labels)\n",
        "    step_counter +=1\n",
        "  \n",
        "    if step_counter % 100 == 0:\n",
        "      print('Step #%d\\tLoss: %.4f' % (step_counter, loss_value))\n",
        "\n",
        "  test_accuracy = compute_accuracy(model(x_test), y_test)\n",
        "  print('Accuracy #%.2f\\n' % (test_accuracy))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}